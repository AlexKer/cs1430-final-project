{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtvt9ICj-Pzd"
      },
      "source": [
        "# **Homework 5: Convolutional Neural Networks**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T5_FovLIMrj"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neOh6Aqppzoz"
      },
      "source": [
        "### **Change Colab's Runtime**\n",
        "\n",
        "Colab provides machine instances in the cloud. We will use Colab to speed up training times via its GPUs.\n",
        "By default, Colab instances do not use GPUs. So, we must first enable GPU support.\n",
        "\n",
        "1. Click the \"Runtime\" menu above\n",
        "2. Click \"Change runtime type\"\n",
        "3. Select \"GPU\" under \"Hardware accelerator\"\n",
        "4. Click Save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blqZxMe3kSU_"
      },
      "source": [
        "## **Linking Github and Google Colab**\n",
        "\n",
        "We're going to create a public/private key pair to allow Colab to access your Github.\n",
        "\n",
        "1. Make sure your latest code is pushed to github.com\n",
        "2. Run the cell below by clicking the 'Play' button to the top left.\n",
        "  *   Google Colab will start a machine instance in the cloud (top right of Colab under 'Share')\n",
        "  *   When the script asks for the location, click the flashing cursor and press enter\n",
        "  *   Leave the password field blank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1nTcPUxcz36"
      },
      "outputs": [],
      "source": [
        "!ssh-keygen -t rsa\n",
        "!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n",
        "!clear\n",
        "!echo \"PUBLIC KEY: (for github.com)\"\n",
        "!cat /root/.ssh/id_rsa.pub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8XJeOF6ihxs"
      },
      "source": [
        "Follow these steps before proceeding:\n",
        "\n",
        "1. Go to https://github.com/settings/keys and login if necessary\n",
        "2. Click the green \"New SSH Key\" button\n",
        "3. Choose any title\n",
        "4. From the cell above in this Colab notebook, copy the public key. It looks like \"ssh-rsa xxxxx\", is very long, and should be the last line.\n",
        "5. Paste the key into the \"key\" text box and click \"Add SSH key\"\n",
        "6. Below, replace **[GITHUBUSERNAME]** with your GitHub username.\n",
        "7. Run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia2CRSzTObA3"
      },
      "outputs": [],
      "source": [
        "!git clone git@github.com:BrownCSCI1430/homework5_cnns-[GITHUBUSERNAME].git\n",
        "%cd homework5_cnns-[GITHUBUSERNAME]\n",
        "!git config --global user.email \"colab_bot@brown.edu\"\n",
        "!git config --global user.name \"Colab Bot\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb1ead5oTA4j"
      },
      "source": [
        "Next, note the left-hand side bar.\n",
        "Find and click on the \"Files\" button on the left; it looks like a file divider. Now, you should be able to see your homework5_cnn folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6LZuGXHR_A6"
      },
      "source": [
        "## **Editing Your Code**\n",
        "\n",
        "Your code on Colab must be pushed back to Github to reflect changes. Thus, we recommend changing your code and tuning parameters **either locally or on Colab, but not both simultaneously**.\n",
        "This will help avoid merge conflicts, which will happen when you make changes without ensuring that you have an up-to-date copy of the code.\n",
        "\n",
        "#### Workflow 1\n",
        "\n",
        "1. Make change locally in `vscode`\n",
        "2. git commit+push to Github in your terminal\n",
        "3. git pull from Github into the Colab copy of your project\n",
        "4. Execute code in Colab\n",
        "\n",
        "#### Workflow 2\n",
        "\n",
        "1. Make changes to your project files on Colab\n",
        "2. git commit+push to Github using Colab cells\n",
        "3. git pull from Github into your local directory on your laptop filesystem\n",
        "4. Edit in `vscode`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMgVf5msR_A6"
      },
      "source": [
        "### **Workflow 1: Making changes locally**\n",
        "\n",
        "If you choose to change your files locally, **DO NOT repeatedly clone the repo**. Once you pushed from local terminal, you can update your code on colab using the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EycXfFp1R_A6"
      },
      "outputs": [],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEsQpamvR_A7"
      },
      "source": [
        "### **Workflow 2: Making changes on Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0ka-a6HR_A7"
      },
      "source": [
        "If you choose to change your files on Colab, you can access them from the files section of the left sidebar. You may double click on the file you wish to edit and open it on colab.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=15fusCIFNpGiibzg5FWod4o8DlpjNHJMZ\" alt=\"colab folder\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn28-QpnR_A7"
      },
      "source": [
        "You can use the following commands to push your code changes to github from colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZcbQb0DR_A7"
      },
      "outputs": [],
      "source": [
        "!git add code/*\n",
        "!git commit -m \"updating code from colab!\"\n",
        "!git push"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtdqxGYWR_A7"
      },
      "source": [
        "### **Installation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL-K_D6HR_A7"
      },
      "source": [
        "We can now install and import the necessary components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "O1jkvao_L68C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lime in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (0.2.0.1)\n",
            "Requirement already satisfied: matplotlib in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from lime) (3.5.3)\n",
            "Requirement already satisfied: numpy in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from lime) (1.23.2)\n",
            "Requirement already satisfied: scipy in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from lime) (1.9.1)\n",
            "Requirement already satisfied: tqdm in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from lime) (4.64.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from lime) (1.1.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from lime) (0.19.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from scikit-image>=0.12->lime) (9.2.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from scikit-image>=0.12->lime) (2.34.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from scikit-image>=0.12->lime) (2023.8.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from scikit-image>=0.12->lime) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from scikit-image>=0.12->lime) (24.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from scikit-learn>=0.18->lime) (3.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from matplotlib->lime) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from matplotlib->lime) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from matplotlib->lime) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from matplotlib->lime) (2.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "85EGz_FZH434"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import \\\n",
        "    Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvfht1-w-35H"
      },
      "source": [
        "Next, we can download the weights for the VGG model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k5rkcA4eDD9o"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-05-10 02:34:23--  https://browncsci1430.github.io/hw5_cnns/vgg16_imagenet.h5\n",
            "Resolving browncsci1430.github.io (browncsci1430.github.io)... 2606:50c0:8003::153, 2606:50c0:8001::153, 2606:50c0:8000::153, ...\n",
            "Connecting to browncsci1430.github.io (browncsci1430.github.io)|2606:50c0:8003::153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58909648 (56M) [application/octet-stream]\n",
            "Saving to: ‘vgg16_imagenet.h5.1’\n",
            "\n",
            "vgg16_imagenet.h5.1 100%[===================>]  56.18M  64.7MB/s    in 0.9s    \n",
            "\n",
            "2024-05-10 02:34:25 (64.7 MB/s) - ‘vgg16_imagenet.h5.1’ saved [58909648/58909648]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://browncsci1430.github.io/hw5_cnns/vgg16_imagenet.h5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hASqy75_Sk7"
      },
      "source": [
        "## **Run your model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h8sTZatR_A7"
      },
      "source": [
        "First, navigate to `code` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF1J5KDdTMHX"
      },
      "outputs": [],
      "source": [
        "%cd code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH43pgcDR_A7"
      },
      "source": [
        "Use the provided `main.py` to train and test your models. The provided `argparse` will automatically log and checkpoint your models in newly created `logs/`, `checkpoints/your_model` and `checkpoints/vgg_model` directories. Run the following command to see all available command line arguments.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wQAu7WnaR_A8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: main.py [-h] --task {1,3} [--data DATA] [--load-vgg LOAD_VGG]\n",
            "               [--load-checkpoint LOAD_CHECKPOINT] [--confusion] [--evaluate]\n",
            "               [--lime-image LIME_IMAGE]\n",
            "\n",
            "Let's train some neural nets!\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --task {1,3}          Which task of the assignment to run - training from\n",
            "                        scratch (1), or fine tuning VGG-16 (3). (default:\n",
            "                        None)\n",
            "  --data DATA           Location where the dataset is stored. (default:\n",
            "                        ../data/)\n",
            "  --load-vgg LOAD_VGG   Path to pre-trained VGG-16 file (only applicable to\n",
            "                        task 3). (default: vgg16_imagenet.h5)\n",
            "  --load-checkpoint LOAD_CHECKPOINT\n",
            "                        Path to model checkpoint file (should end with the\n",
            "                        extension .h5). Checkpoints are automatically saved\n",
            "                        when you train your model. If you want to continue\n",
            "                        training from where you left off, this is how you\n",
            "                        would load your weights. (default: None)\n",
            "  --confusion           Log a confusion matrix at the end of each epoch\n",
            "                        (viewable in Tensorboard). This is turned off by\n",
            "                        default as it takes a little bit of time to complete.\n",
            "                        (default: False)\n",
            "  --evaluate            Skips training and evaluates on the test set once. You\n",
            "                        can use this to test an already trained model by\n",
            "                        loading its checkpoint. (default: False)\n",
            "  --lime-image LIME_IMAGE\n",
            "                        Name of an image in the dataset to use for LIME\n",
            "                        evaluation. (default:\n",
            "                        ../data/test/Bedroom/image_0003.jpg)\n"
          ]
        }
      ],
      "source": [
        "!python main.py -h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZpIwCwxR_A8"
      },
      "source": [
        "### **Running the tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCNrNsw-R_A8"
      },
      "source": [
        "#### **Task 1 and 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayfihZsqR_A8"
      },
      "source": [
        "The following section uses task 1 as an example. You should be able to run task 3 by replacing `task 1` with `task 3`, and `your_model` with `vgg_model` in the following section.\n",
        "\n",
        "Use the following command to run a task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aE8BEyNf_jzL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset mean shape: [48, 48, 3]\n",
            "Dataset mean top left pixel value: [0.4900, 0.4900, 0.4900]\n",
            "Dataset std shape: [48, 48, 3]\n",
            "Dataset std top left pixel value: [0.3248, 0.3248, 0.3248]\n",
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "Model: \"vgg_base\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Model: \"vgg_head\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               6422784   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 7)                 903       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,456,583\n",
            "Trainable params: 6,456,583\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Done setting up image labeling logger.\n",
            "Epoch 1/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.8599 - sparse_categorical_accuracy: 0.3066\n",
            "Epoch 001 TEST accuracy (0.4133) EXCEEDED previous maximum TEST accuracy.\n",
            "Saving checkpoint at checkpoints/vgg_model/051024-023516//vgg.weights.e000-acc0.4133.h5\n",
            "2871/2871 [==============================] - 2926s 1s/step - loss: 1.8599 - sparse_categorical_accuracy: 0.3066 - val_loss: 1.5558 - val_sparse_categorical_accuracy: 0.4133\n",
            "Epoch 2/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.5901 - sparse_categorical_accuracy: 0.3753\n",
            "Epoch 002 TEST accuracy (0.4479) EXCEEDED previous maximum TEST accuracy.\n",
            "Saving checkpoint at checkpoints/vgg_model/051024-023516//vgg.weights.e001-acc0.4479.h5\n",
            "2871/2871 [==============================] - 2786s 970ms/step - loss: 1.5901 - sparse_categorical_accuracy: 0.3753 - val_loss: 1.4341 - val_sparse_categorical_accuracy: 0.4479\n",
            "Epoch 3/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.5295 - sparse_categorical_accuracy: 0.4012\n",
            "Epoch 003 TEST accuracy (0.4586) EXCEEDED previous maximum TEST accuracy.\n",
            "Saving checkpoint at checkpoints/vgg_model/051024-023516//vgg.weights.e002-acc0.4586.h5\n",
            "2871/2871 [==============================] - 2796s 974ms/step - loss: 1.5295 - sparse_categorical_accuracy: 0.4012 - val_loss: 1.4129 - val_sparse_categorical_accuracy: 0.4586\n",
            "Epoch 4/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.4916 - sparse_categorical_accuracy: 0.4204\n",
            "Epoch 004 TEST accuracy (0.4581) DID NOT EXCEED previous maximum TEST accuracy.\n",
            "No checkpoint was saved\n",
            "2871/2871 [==============================] - 2796s 974ms/step - loss: 1.4916 - sparse_categorical_accuracy: 0.4204 - val_loss: 1.4154 - val_sparse_categorical_accuracy: 0.4581\n",
            "Epoch 5/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.4648 - sparse_categorical_accuracy: 0.4345\n",
            "Epoch 005 TEST accuracy (0.4529) DID NOT EXCEED previous maximum TEST accuracy.\n",
            "No checkpoint was saved\n",
            "2871/2871 [==============================] - 2778s 968ms/step - loss: 1.4648 - sparse_categorical_accuracy: 0.4345 - val_loss: 1.4046 - val_sparse_categorical_accuracy: 0.4529\n",
            "Epoch 6/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.4479 - sparse_categorical_accuracy: 0.4415\n",
            "Epoch 006 TEST accuracy (0.4638) EXCEEDED previous maximum TEST accuracy.\n",
            "Saving checkpoint at checkpoints/vgg_model/051024-023516//vgg.weights.e005-acc0.4638.h5\n",
            "2871/2871 [==============================] - 2800s 975ms/step - loss: 1.4479 - sparse_categorical_accuracy: 0.4415 - val_loss: 1.3915 - val_sparse_categorical_accuracy: 0.4638\n",
            "Epoch 7/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.4343 - sparse_categorical_accuracy: 0.4466\n",
            "Epoch 007 TEST accuracy (0.4726) EXCEEDED previous maximum TEST accuracy.\n",
            "Saving checkpoint at checkpoints/vgg_model/051024-023516//vgg.weights.e006-acc0.4726.h5\n",
            "2871/2871 [==============================] - 2788s 971ms/step - loss: 1.4343 - sparse_categorical_accuracy: 0.4466 - val_loss: 1.3721 - val_sparse_categorical_accuracy: 0.4726\n",
            "Epoch 8/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.4344 - sparse_categorical_accuracy: 0.4455\n",
            "Epoch 008 TEST accuracy (0.4886) EXCEEDED previous maximum TEST accuracy.\n",
            "Saving checkpoint at checkpoints/vgg_model/051024-023516//vgg.weights.e007-acc0.4886.h5\n",
            "2871/2871 [==============================] - 2800s 975ms/step - loss: 1.4344 - sparse_categorical_accuracy: 0.4455 - val_loss: 1.3467 - val_sparse_categorical_accuracy: 0.4886\n",
            "Epoch 9/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.4164 - sparse_categorical_accuracy: 0.4541\n",
            "Epoch 009 TEST accuracy (0.4848) DID NOT EXCEED previous maximum TEST accuracy.\n",
            "No checkpoint was saved\n",
            "2871/2871 [==============================] - 2874s 1s/step - loss: 1.4164 - sparse_categorical_accuracy: 0.4541 - val_loss: 1.3644 - val_sparse_categorical_accuracy: 0.4848\n",
            "Epoch 10/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.4046 - sparse_categorical_accuracy: 0.4592\n",
            "Epoch 010 TEST accuracy (0.4778) DID NOT EXCEED previous maximum TEST accuracy.\n",
            "No checkpoint was saved\n",
            "2871/2871 [==============================] - 3327s 1s/step - loss: 1.4046 - sparse_categorical_accuracy: 0.4592 - val_loss: 1.3531 - val_sparse_categorical_accuracy: 0.4778\n",
            "Epoch 11/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.3916 - sparse_categorical_accuracy: 0.4631\n",
            "Epoch 011 TEST accuracy (0.4872) DID NOT EXCEED previous maximum TEST accuracy.\n",
            "No checkpoint was saved\n",
            "2871/2871 [==============================] - 2954s 1s/step - loss: 1.3916 - sparse_categorical_accuracy: 0.4631 - val_loss: 1.3438 - val_sparse_categorical_accuracy: 0.4872\n",
            "Epoch 12/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.3872 - sparse_categorical_accuracy: 0.4642\n",
            "Epoch 012 TEST accuracy (0.4901) EXCEEDED previous maximum TEST accuracy.\n",
            "Saving checkpoint at checkpoints/vgg_model/051024-023516//vgg.weights.e011-acc0.4901.h5\n",
            "2871/2871 [==============================] - 3085s 1s/step - loss: 1.3872 - sparse_categorical_accuracy: 0.4642 - val_loss: 1.3403 - val_sparse_categorical_accuracy: 0.4901\n",
            "Epoch 13/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.3829 - sparse_categorical_accuracy: 0.4677\n",
            "Epoch 013 TEST accuracy (0.4876) DID NOT EXCEED previous maximum TEST accuracy.\n",
            "No checkpoint was saved\n",
            "2871/2871 [==============================] - 3288s 1s/step - loss: 1.3829 - sparse_categorical_accuracy: 0.4677 - val_loss: 1.3416 - val_sparse_categorical_accuracy: 0.4876\n",
            "Epoch 14/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.3771 - sparse_categorical_accuracy: 0.4698\n",
            "Epoch 014 TEST accuracy (0.4923) EXCEEDED previous maximum TEST accuracy.\n",
            "Saving checkpoint at checkpoints/vgg_model/051024-023516//vgg.weights.e013-acc0.4923.h5\n",
            "2871/2871 [==============================] - 3302s 1s/step - loss: 1.3771 - sparse_categorical_accuracy: 0.4698 - val_loss: 1.3423 - val_sparse_categorical_accuracy: 0.4923\n",
            "Epoch 15/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.3759 - sparse_categorical_accuracy: 0.4718\n",
            "Epoch 015 TEST accuracy (0.4964) EXCEEDED previous maximum TEST accuracy.\n",
            "Saving checkpoint at checkpoints/vgg_model/051024-023516//vgg.weights.e014-acc0.4964.h5\n",
            "2871/2871 [==============================] - 3039s 1s/step - loss: 1.3759 - sparse_categorical_accuracy: 0.4718 - val_loss: 1.3187 - val_sparse_categorical_accuracy: 0.4964\n",
            "Epoch 16/50\n",
            "2871/2871 [==============================] - ETA: 0s - loss: 1.3713 - sparse_categorical_accuracy: 0.4718\n",
            "Epoch 016 TEST accuracy (0.4912) DID NOT EXCEED previous maximum TEST accuracy.\n",
            "No checkpoint was saved\n",
            "2871/2871 [==============================] - 3390s 1s/step - loss: 1.3713 - sparse_categorical_accuracy: 0.4718 - val_loss: 1.3258 - val_sparse_categorical_accuracy: 0.4912\n",
            "Epoch 17/50\n",
            " 368/2871 [==>...........................] - ETA: 39:13 - loss: 1.3691 - sparse_categorical_accuracy: 0.4776^C\n"
          ]
        }
      ],
      "source": [
        "!python main.py --task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvlbuHbMR_A8"
      },
      "source": [
        "**Choosing a checkpoint**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MieUEyQdR_A8"
      },
      "source": [
        "You can load from a specific checkpoint if your current model architecture is the **same** as your model at that checkpoint.\n",
        "\n",
        "View your checkpoints timestamps and pick one from the list below. The last folder in the list contains your latest training results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9IVTfEzZR_A8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[36m051024-023149\u001b[m\u001b[m/ \u001b[1m\u001b[36m051024-023516\u001b[m\u001b[m/\n"
          ]
        }
      ],
      "source": [
        "%ls checkpoints/vgg_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIPSDL4NR_A8"
      },
      "source": [
        "Replace [TIMESTAMP] below with the checkpoint timestamp you've chosen to get a list of model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0XqOq4hIR_A8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vgg.weights.e006-acc0.4726.h5  vgg.weights.e013-acc0.4923.h5\n",
            "vgg.weights.e007-acc0.4886.h5  vgg.weights.e014-acc0.4964.h5\n",
            "vgg.weights.e011-acc0.4901.h5\n"
          ]
        }
      ],
      "source": [
        "# TODO: replace [TIMESTAMP] with the timestamp you choose from the last step\n",
        "%ls checkpoints/vgg_model/051024-023516"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Ao-wNMR_A8"
      },
      "source": [
        "Choose a model from the list above and replace [TIMESTAMP]/[MODEL_FILENAME] with your timestamp and checkpoint file. The last file contains your latest training weights. If you want, you can use the following command to continue training the model with specific weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mbf_MSkpR_A8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset mean shape: [48, 48, 3]\n",
            "Dataset mean top left pixel value: [0.4605, 0.4605, 0.4605]\n",
            "Dataset std shape: [48, 48, 3]\n",
            "Dataset std top left pixel value: [0.3135, 0.3135, 0.3135]\n",
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "Model: \"vgg_base\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Model: \"vgg_head\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               6422784   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 7)                 903       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,456,583\n",
            "Trainable params: 6,456,583\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Done setting up image labeling logger.\n",
            "Epoch 16/50\n",
            "  1/225 [..............................] - ETA: 51:46 - loss: 1.3316 - sparse_categorical_accuracy: 0.5156^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/seikoh/BrownWorkspace/CS1430_Projects/cs1430-final-project/code/main.py\", line 273, in <module>\n",
            "    main()\n",
            "  File \"/Users/seikoh/BrownWorkspace/CS1430_Projects/cs1430-final-project/code/main.py\", line 267, in main\n",
            "    train(model, datasets, checkpoint_path, logs_path, init_epoch)\n",
            "  File \"/Users/seikoh/BrownWorkspace/CS1430_Projects/cs1430-final-project/code/main.py\", line 166, in train\n",
            "    model.fit(\n",
            "  File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/keras/engine/training.py\", line 1409, in fit\n",
            "    tmp_logs = self.train_function(iterator)\n",
            "  File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 915, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 947, in _call\n",
            "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 2453, in __call__\n",
            "    return graph_function._call_flat(\n",
            "  File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 1860, in _call_flat\n",
            "    return self._build_call_outputs(self._inference_function.call(\n",
            "  File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 497, in call\n",
            "    outputs = execute.execute(\n",
            "  File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\", line 54, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# TODO: replace [TIMESTAMP][MODEL_FILENAME] with the a timestamp from above list\n",
        "!python main.py --task 3 --load-checkpoint checkpoints/vgg_model/051024-023516/vgg.weights.e014-acc0.4964.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuBtiLTSR_A8"
      },
      "source": [
        "**Testing your model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nei4Dwb3R_A8"
      },
      "source": [
        "Run the following command to evaluate your model. Follow the \"choosing your model\" section to replace [TIMESTAMP][MODEL_FILENAME]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fUZHoSvUIeTt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset mean shape: [48, 48, 3]\n",
            "Dataset mean top left pixel value: [0.4732, 0.4732, 0.4732]\n",
            "Dataset std shape: [48, 48, 3]\n",
            "Dataset std top left pixel value: [0.3286, 0.3286, 0.3286]\n",
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "Model: \"vgg_base\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Model: \"vgg_head\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               6422784   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 7)                 903       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,456,583\n",
            "Trainable params: 6,456,583\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "57/57 [==============================] - 764s 13s/step - loss: 1.3187 - sparse_categorical_accuracy: 0.4964\n",
            "  1%|▍                                         | 9/1000 [00:00<00:20, 48.19it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/seikoh/BrownWorkspace/CS1430_Projects/cs1430-final-project/code/main.py\", line 273, in <module>\n",
            "    main()\n",
            "  File \"/Users/seikoh/BrownWorkspace/CS1430_Projects/cs1430-final-project/code/main.py\", line 265, in main\n",
            "    LIME_explainer(model, path, datasets.preprocess_fn, timestamp)\n",
            "  File \"/Users/seikoh/BrownWorkspace/CS1430_Projects/cs1430-final-project/code/main.py\", line 117, in LIME_explainer\n",
            "    explanation = explainer.explain_instance(\n",
            "  File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/lime/lime_image.py\", line 198, in explain_instance\n",
            "    data, labels = self.data_labels(image, fudged_image, segments,\n",
            "  File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/lime/lime_image.py\", line 261, in data_labels\n",
            "    preds = classifier_fn(np.array(imgs))\n",
            "  File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/var/folders/w8/dj_5l1q12kb_0k40b24m1gwr0000gq/T/__autograph_generated_filelqyeipfv.py\", line 15, in tf__predict_function\n",
            "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
            "  File \"/var/folders/w8/dj_5l1q12kb_0k40b24m1gwr0000gq/T/__autograph_generated_fileuw6ec1u5.py\", line 12, in tf__call\n",
            "    x = ag__.converted_call(ag__.ld(self).head, (ag__.ld(x),), None, fscope)\n",
            "ValueError: in user code:\n",
            "\n",
            "    File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/keras/engine/training.py\", line 1845, in predict_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/keras/engine/training.py\", line 1834, in step_function  **\n",
            "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/keras/engine/training.py\", line 1823, in run_step  **\n",
            "        outputs = model.predict_step(data)\n",
            "    File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/keras/engine/training.py\", line 1791, in predict_step\n",
            "        return self(x, training=False)\n",
            "    File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "        raise e.with_traceback(filtered_tb) from None\n",
            "    File \"/var/folders/w8/dj_5l1q12kb_0k40b24m1gwr0000gq/T/__autograph_generated_fileuw6ec1u5.py\", line 12, in tf__call\n",
            "        x = ag__.converted_call(ag__.ld(self).head, (ag__.ld(x),), None, fscope)\n",
            "\n",
            "    ValueError: Exception encountered when calling layer \"vgg_model\" (type VGGModel).\n",
            "    \n",
            "    in user code:\n",
            "    \n",
            "        File \"/Users/seikoh/BrownWorkspace/CS1430_Projects/cs1430-final-project/code/models.py\", line 180, in call  *\n",
            "            x = self.head(x)\n",
            "        File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n",
            "            raise e.with_traceback(filtered_tb) from None\n",
            "        File \"/Users/seikoh/anaconda3/envs/cs1430/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 248, in assert_input_compatibility\n",
            "            raise ValueError(\n",
            "    \n",
            "        ValueError: Exception encountered when calling layer \"vgg_head\" (type Sequential).\n",
            "        \n",
            "        Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 25088, but received input with shape (None, 512)\n",
            "        \n",
            "        Call arguments received by layer \"vgg_head\" (type Sequential):\n",
            "          • inputs=tf.Tensor(shape=(None, 1, 1, 512), dtype=float32)\n",
            "          • training=False\n",
            "          • mask=None\n",
            "    \n",
            "    \n",
            "    Call arguments received by layer \"vgg_model\" (type VGGModel):\n",
            "      • x=tf.Tensor(shape=(None, 48, 48, 3), dtype=float32)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: replace [TIMESTAMP][MODEL_FILENAME] with the a checkpoint\n",
        "!python main.py --task 3 --load-checkpoint checkpoints/vgg_model/051024-023516/vgg.weights.e014-acc0.4964.h5 --evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddBRF-2XR_BB"
      },
      "source": [
        "#### **Task 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBOR9JZM2Tej"
      },
      "source": [
        "\n",
        "**View Tensorboard Logs**\n",
        "\n",
        "- **Scalars** show how the loss and metrics change with every epoch. You can use them to also track training speed, learning rate, and other scalar values. Scalars can be found in the **Time Series** or **Scalars** dashboards.\n",
        "- **Graphs** help you visualize your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly. Graphs can be found in the **Graphs** dashboard.\n",
        "- **Histograms** and **Distributions** show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way. Histograms can be found in the **Time Series** or **Histograms** dashboards. Distributions can be found in the **Distributions** dashboard.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Wt1-9hiB2XZU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 31718), started 1:14:52 ago. (Use '!kill 31718' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-570c91a8d46eee28\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-570c91a8d46eee28\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgspF9W_R_BC"
      },
      "source": [
        "**Lime Interpreter**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUDxmxcHstIM"
      },
      "source": [
        "Choose an image your model falsely classified and add it to the end of the command below (replace misclassified/your_model/Bedroom/Store_predicted.png with the path to your image)\n",
        "\n",
        "\n",
        "**Where to find misclassified images?**\n",
        "- Under your code folder, find folder **misclassfied**. In the folder you can look for the misclassifed images in their groundtruth category folder, with their names explaining what they are misclassified as. Pick an example you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vk-mNdSXIrnF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset mean shape: [48, 48, 3]\n",
            "Dataset mean top left pixel value: [0.4619, 0.4619, 0.4619]\n",
            "Dataset std shape: [48, 48, 3]\n",
            "Dataset std top left pixel value: [0.3294, 0.3294, 0.3294]\n",
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "Model: \"vgg_base\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Model: \"vgg_head\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               6422784   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 7)                 903       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,456,583\n",
            "Trainable params: 6,456,583\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            " 2/57 [>.............................] - ETA: 10:27 - loss: 1.6982 - sparse_categorical_accuracy: 0.2812"
          ]
        }
      ],
      "source": [
        "# TODO: replace the image path with the falsely classified image name\n",
        "# TODO: replace [TIMESTAMP][MODEL_FILENAME] with the a checkpoint\n",
        "# !python main.py --task 3 --load-checkpoint checkpoints/vgg_model/051024-023516/vgg.weights.e014-acc0.4964.h5 --evaluate --lime-image misclassified/vgg_model/data/surprised_predicted.png\n",
        "!python main.py --task 3 --load-checkpoint checkpoints/vgg_model/051024-023516/vgg.weights.e014-acc0.4964.h5 --evaluate --lime-image /misclassified/vgg_model/051024-023516/angry/fearful_predicted.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHXw7vD8R_BC"
      },
      "source": [
        "**[Optional] Display Lime Interpreter Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enrNzg40R_BC"
      },
      "outputs": [],
      "source": [
        "# In order to see the images, replace <TIMESTAMP> with your own timestamp\n",
        "for i in range(4):\n",
        "  plot_img = imread('lime_explainer_images/051024-023516/' + str(i) + '.png')\n",
        "  plt.imshow(plot_img)\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
